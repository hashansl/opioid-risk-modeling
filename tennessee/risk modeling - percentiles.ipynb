{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tennessee opioid risk modeling \n",
    "Normal adjacency matrix - Dimentions would be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'DC': ['EP_MUNIT', selected_variables_tn[0], 'EP_MINRTY', 'EP_AGE65', 'EP_CROWD'], 'TN': [selected_variables_tn[0], 'EP_NOHSDP', 'EP_PCI', 'EP_MOBILE', 'EP_POV'], 'WY': [selected_variables_tn[0], 'EP_PCI', 'EP_LIMENG', 'EP_CROWD', 'EP_UNINSUR']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pickle as pickle\n",
    "from pylab import *\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_svi_nod = gpd.read_file('./data/processed data/SVI with HepVu census tracts/SVI2018 TN census tracts with death rate HepVu/SVI2018_TN_census_tracts_with_death_rate_HepVu.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering the raw variables with overdose death rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = ['EP_POV','EP_UNEMP','EP_PCI','EP_NOHSDP','EP_UNINSUR','EP_AGE65','EP_AGE17','EP_DISABL','EP_SNGPNT','EP_LIMENG','EP_MINRTY','EP_MUNIT','EP_MOBILE','EP_CROWD','EP_NOVEH','EP_GROUPQ','NOD_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_without_y = ['EP_POV','EP_UNEMP','EP_PCI','EP_NOHSDP','EP_UNINSUR','EP_AGE65','EP_AGE17','EP_DISABL','EP_SNGPNT','EP_LIMENG','EP_MINRTY','EP_MUNIT','EP_MOBILE','EP_CROWD','EP_NOVEH','EP_GROUPQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_tn_with_geo = ['FIPS','EP_DISABL', 'EP_NOHSDP', 'EP_PCI', 'EP_MOBILE', 'EP_POV','NOD_Rate','geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_tn = ['EP_DISABL', 'EP_NOHSDP', 'EP_PCI', 'EP_MOBILE', 'EP_POV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_tn_with_od = ['EP_DISABL', 'EP_NOHSDP', 'EP_PCI', 'EP_MOBILE', 'EP_POV','NOD_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of rows with 999 or -999 values in the selected_variablesWy_ columns\n",
    "rows_with_n999 = tn_svi_nod[(tn_svi_nod['EP_DISABL'] == -999) | (tn_svi_nod['EP_NOHSDP'] == -999) | (tn_svi_nod['EP_PCI'] == -999) | (tn_svi_nod['EP_MOBILE'] == -999) | (tn_svi_nod['EP_POV'] == -999)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_n999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if value equals -999, replace with 0 in selected_variablesWy_\n",
    "tn_svi_nod[selected_variables_tn[0]] = tn_svi_nod[selected_variables_tn[0]].replace(-999, 0)\n",
    "tn_svi_nod[selected_variables_tn[1]] = tn_svi_nod[selected_variables_tn[1]].replace(-999, 0)\n",
    "tn_svi_nod[selected_variables_tn[2]] = tn_svi_nod[selected_variables_tn[2]].replace(-999, 0)\n",
    "tn_svi_nod[selected_variables_tn[3]] = tn_svi_nod[selected_variables_tn[3]].replace(-999, 0)\n",
    "tn_svi_nod[selected_variables_tn[4]] = tn_svi_nod[selected_variables_tn[4]].replace(-999, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_svi_nod[selected_variables_tn_with_geo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the distribution of each selected variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_svi_nod[selected_variables_tn].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))\n",
    "for i, ax in zip(selected_variables_tn, axes.flatten()):\n",
    "    tn_svi_nod[i].plot.hist(bins=20, alpha=0.5, ax=ax, edgecolor='black')\n",
    "    ax.set_title(i, fontsize=12)  # Adjust the fontsize here\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the 50th and 75th and 90th percentile for each variable\n",
    "variable_1_percentile_50 = tn_svi_nod[selected_variables_tn[0]].quantile(0.5)\n",
    "variable_1_percentile_75 = tn_svi_nod[selected_variables_tn[0]].quantile(0.75)\n",
    "variable_1_percentile_90 = tn_svi_nod[selected_variables_tn[0]].quantile(0.9)\n",
    "\n",
    "variable_2_percentile_50 = tn_svi_nod[selected_variables_tn[1]].quantile(0.5)\n",
    "variable_2_percentile_75 = tn_svi_nod[selected_variables_tn[1]].quantile(0.75)\n",
    "variable_2_percentile_90 = tn_svi_nod[selected_variables_tn[1]].quantile(0.9)\n",
    "\n",
    "variable_3_percentile_50 = tn_svi_nod[selected_variables_tn[2]].quantile(0.5)\n",
    "variable_3_percentile_75 = tn_svi_nod[selected_variables_tn[2]].quantile(0.75)\n",
    "variable_3_percentile_90 = tn_svi_nod[selected_variables_tn[2]].quantile(0.9)\n",
    "\n",
    "variable_4_percentile_50 = tn_svi_nod[selected_variables_tn[3]].quantile(0.5)\n",
    "variable_4_percentile_75 = tn_svi_nod[selected_variables_tn[3]].quantile(0.75)\n",
    "variable_4_percentile_90 = tn_svi_nod[selected_variables_tn[3]].quantile(0.9)\n",
    "\n",
    "variable_5_percentile_50 = tn_svi_nod[selected_variables_tn[4]].quantile(0.5)\n",
    "variable_5_percentile_75 = tn_svi_nod[selected_variables_tn[4]].quantile(0.75)\n",
    "variable_5_percentile_90 = tn_svi_nod[selected_variables_tn[4]].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each selected vcariable wy calculate the number of rows that are below 50th, 75th and 90th percentile\n",
    "variable_1_below_50 = tn_svi_nod[tn_svi_nod[selected_variables_tn[0]] < variable_1_percentile_50].shape[0]\n",
    "variable_1_below_75 = tn_svi_nod[tn_svi_nod[selected_variables_tn[0]] < variable_1_percentile_75].shape[0]\n",
    "variable_1_below_90 = tn_svi_nod[tn_svi_nod[selected_variables_tn[0]] < variable_1_percentile_90].shape[0]\n",
    "\n",
    "variable_2_below_50 = tn_svi_nod[tn_svi_nod[selected_variables_tn[1]] < variable_2_percentile_50].shape[0]\n",
    "variable_2_below_75 = tn_svi_nod[tn_svi_nod[selected_variables_tn[1]] < variable_2_percentile_75].shape[0]\n",
    "variable_2_below_90 = tn_svi_nod[tn_svi_nod[selected_variables_tn[1]] < variable_2_percentile_90].shape[0]\n",
    "\n",
    "variable_3_below_50 = tn_svi_nod[tn_svi_nod[selected_variables_tn[2]] < variable_3_percentile_50].shape[0]\n",
    "variable_3_below_75 = tn_svi_nod[tn_svi_nod[selected_variables_tn[2]] < variable_3_percentile_75].shape[0]\n",
    "variable_3_below_90 = tn_svi_nod[tn_svi_nod[selected_variables_tn[2]] < variable_3_percentile_90].shape[0]\n",
    "\n",
    "variable_4_below_50 = tn_svi_nod[tn_svi_nod[selected_variables_tn[3]] < variable_4_percentile_50].shape[0]\n",
    "variable_4_below_75 = tn_svi_nod[tn_svi_nod[selected_variables_tn[3]] < variable_4_percentile_75].shape[0]\n",
    "variable_4_below_90 = tn_svi_nod[tn_svi_nod[selected_variables_tn[3]] < variable_4_percentile_90].shape[0]\n",
    "\n",
    "variable_5_below_50 = tn_svi_nod[tn_svi_nod[selected_variables_tn[4]] < variable_5_percentile_50].shape[0]\n",
    "variable_5_below_75 = tn_svi_nod[tn_svi_nod[selected_variables_tn[4]] < variable_5_percentile_75].shape[0]\n",
    "variable_5_below_90 = tn_svi_nod[tn_svi_nod[selected_variables_tn[4]] < variable_5_percentile_90].shape[0]\n",
    "\n",
    "# create a dataframe with the results\n",
    "data = {'Variable': selected_variables_tn, '50th Percentile': [variable_1_percentile_50, variable_2_percentile_50, variable_3_percentile_50, variable_4_percentile_50, variable_5_percentile_50], '75th Percentile': [variable_1_percentile_75, variable_2_percentile_75, variable_3_percentile_75, variable_4_percentile_75, variable_5_percentile_75], '90th Percentile': [variable_1_percentile_90, variable_2_percentile_90, variable_3_percentile_90, variable_4_percentile_90, variable_5_percentile_90], 'Rows below 50th Percentile': [variable_1_below_50, variable_2_below_50, variable_3_below_50, variable_4_below_50, variable_5_below_50], 'Rows below 75th Percentile': [variable_1_below_75, variable_2_below_75, variable_3_below_75, variable_4_below_75, variable_5_below_75], 'Rows below 90th Percentile': [variable_1_below_90, variable_2_below_90, variable_3_below_90, variable_4_below_90, variable_5_below_90]}\n",
    "percentile_df = pd.DataFrame(data)\n",
    "\n",
    "percentile_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get number of rows below and above mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EP_DISABL\n",
    "variable_1_mean = tn_svi_nod[selected_variables_tn[0]].mean()\n",
    "variable_1_sd = tn_svi_nod[selected_variables_tn[0]].std()\n",
    "\n",
    "# EP_PCI \n",
    "variable_2_mean = tn_svi_nod[selected_variables_tn[1]].mean()\n",
    "variable_2_sd = tn_svi_nod[selected_variables_tn[1]].std()\n",
    "\n",
    "# EP_LIMENG\n",
    "variable_3_mean = tn_svi_nod[selected_variables_tn[2]].mean()\n",
    "variable_3_sd = tn_svi_nod[selected_variables_tn[2]].std()\n",
    "\n",
    "# EP_CROWD\n",
    "variable_4_mean = tn_svi_nod[selected_variables_tn[2]].mean()\n",
    "variable_4_sd = tn_svi_nod[selected_variables_tn[2]].std()\n",
    "\n",
    "# EP_UNINSUR\n",
    "variable_5_mean = tn_svi_nod[selected_variables_tn[3]].mean()\n",
    "variable_5_sd = tn_svi_nod[selected_variables_tn[3]].std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each selected vcariable wy calculate the number of rows that are within 1, 2 standard deviations from the mean\n",
    "# EP_DISABL\n",
    "variable_1_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[0]] >= variable_1_mean - variable_1_sd) & (tn_svi_nod[selected_variables_tn[0]] <= variable_1_mean + variable_1_sd)].shape[0]\n",
    "variable_1_2sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[0]] >= variable_1_mean - 2*variable_1_sd) & (tn_svi_nod[selected_variables_tn[0]] <= variable_1_mean + 2*variable_1_sd)].shape[0]\n",
    "\n",
    "# EP_PCI\n",
    "variable_2_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[1]] >= variable_2_mean - variable_2_sd) & (tn_svi_nod[selected_variables_tn[1]] <= variable_2_mean + variable_2_sd)].shape[0]\n",
    "variable_2_2sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[1]] >= variable_2_mean - 2*variable_2_sd) & (tn_svi_nod[selected_variables_tn[1]] <= variable_2_mean + 2*variable_2_sd)].shape[0]\n",
    "\n",
    "# EP_LIMENG\n",
    "variable_3_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[2]] >= variable_3_mean - variable_3_sd) & (tn_svi_nod[selected_variables_tn[2]] <= variable_3_mean + variable_3_sd)].shape[0]\n",
    "variable_3_2sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[2]] >= variable_3_mean - 2*variable_3_sd) & (tn_svi_nod[selected_variables_tn[2]] <= variable_3_mean + 2*variable_3_sd)].shape[0]\n",
    "\n",
    "# EP_CROWD\n",
    "variable_4_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[3]] >= variable_4_mean - variable_4_sd) & (tn_svi_nod[selected_variables_tn[3]] <= variable_4_mean + variable_4_sd)].shape[0]\n",
    "variable_4_2sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[3]] >= variable_4_mean - 2*variable_4_sd) & (tn_svi_nod[selected_variables_tn[3]] <= variable_4_mean + 2*variable_4_sd)].shape[0]\n",
    "\n",
    "# EP_UNINSUR\n",
    "variable_5_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[4]] >= variable_5_mean - variable_5_sd) & (tn_svi_nod[selected_variables_tn[4]] <= variable_5_mean + variable_5_sd)].shape[0]\n",
    "variable_5_2sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[4]] >= variable_5_mean - 2*variable_5_sd) & (tn_svi_nod[selected_variables_tn[4]] <= variable_5_mean + 2*variable_5_sd)].shape[0]\n",
    "\n",
    "# create a dataframe with the results\n",
    "data = {'Variable': [selected_variables_tn[0], 'EP_PCI', selected_variables_tn[2], selected_variables_tn[3], selected_variables_tn[4]], 'Mean': [variable_1_mean, variable_2_mean, variable_3_mean, variable_4_mean, variable_5_mean], 'Standard Deviation': [variable_1_sd, variable_2_sd, variable_3_sd, variable_4_sd, variable_5_sd], 'Within 1 SD': [variable_1_1sd, variable_2_1sd, variable_3_1sd, variable_4_1sd, variable_5_1sd], 'Within 2 SD': [variable_1_2sd, variable_2_2sd, variable_3_2sd, variable_4_2sd, variable_5_2sd]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each selected vcariable wy calculate the number of rows that are below 1 standard deviation from the mean and rows below mean and above 1 standard deviation\n",
    "# EP_DISABL\n",
    "variable_1_below_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[0]] < variable_1_mean - variable_1_sd)].shape[0]\n",
    "variable_1_below_mean = tn_svi_nod[(tn_svi_nod[selected_variables_tn[0]] < variable_1_mean)].shape[0]\n",
    "variable_1_below_mean_plus_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[0]] < variable_1_mean + variable_1_sd)].shape[0]\n",
    "\n",
    "# EP_PCI\n",
    "variable_2_below_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[1]] < variable_2_mean - variable_2_sd)].shape[0]\n",
    "variable_2_below_mean = tn_svi_nod[(tn_svi_nod[selected_variables_tn[1]] < variable_2_mean)].shape[0]\n",
    "variable_2_below_mean_plus_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[1]] < variable_2_mean + variable_2_sd)].shape[0]\n",
    "\n",
    "# EP_LIMENG\n",
    "variable_3_below_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[2]] < variable_3_mean - variable_3_sd)].shape[0]\n",
    "variable_3_below_mean = tn_svi_nod[(tn_svi_nod[selected_variables_tn[2]] < variable_3_mean)].shape[0]\n",
    "variable_3_below_mean_plus_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[2]] < variable_3_mean + variable_3_sd)].shape[0]\n",
    "\n",
    "# EP_CROWD\n",
    "variable_4_below_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[3]] < variable_4_mean - variable_4_sd)].shape[0]\n",
    "variable_4_below_mean = tn_svi_nod[(tn_svi_nod[selected_variables_tn[3]] < variable_4_mean)].shape[0]\n",
    "variable_4_below_mean_plus_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[3]] < variable_4_mean + variable_4_sd)].shape[0]\n",
    "\n",
    "# EP_UNINSUR\n",
    "variable_5_below_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[4]] < variable_5_mean - variable_5_sd)].shape[0]\n",
    "variable_5_below_mean = tn_svi_nod[(tn_svi_nod[selected_variables_tn[4]] < variable_5_mean)].shape[0]\n",
    "variable_5_below_mean_plus_1sd = tn_svi_nod[(tn_svi_nod[selected_variables_tn[4]] < variable_5_mean + variable_5_sd)].shape[0]\n",
    "\n",
    "# create a dataframe with the results\n",
    "data = {'Variable': [selected_variables_tn[0], selected_variables_tn[1], selected_variables_tn[2], selected_variables_tn[3], selected_variables_tn[4]], 'Below Mean-1 SD': [variable_1_below_1sd, variable_2_below_1sd, variable_3_below_1sd, variable_4_below_1sd, variable_5_below_1sd], 'Below Mean': [variable_1_below_mean, variable_2_below_mean, variable_3_below_mean, variable_4_below_mean, variable_5_below_mean], 'Below Mean + 1 SD': [variable_1_below_mean_plus_1sd, variable_2_below_mean_plus_1sd, variable_3_below_mean_plus_1sd, variable_4_below_mean_plus_1sd, variable_5_below_mean_plus_1sd]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "tn_svi_nod[selected_variables_tn[0]].plot.hist(bins=20, alpha=0.8, ax=ax, edgecolor='black')\n",
    "\n",
    "# Axis formatting.\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_color('#DDDDDD')\n",
    "ax.tick_params(bottom=False, left=False)\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(True, color='#EEEEEE')\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add text annotations to the top of the bars.\n",
    "for bar in ax.patches:\n",
    "    bar_value = int(bar.get_height())\n",
    "    text = f'{bar_value:,}'\n",
    "    text_x = bar.get_x() + bar.get_width() / 2\n",
    "    text_y = bar.get_y() + bar_value\n",
    "    bar_color = bar.get_facecolor()\n",
    "    ax.text(text_x, text_y, text, ha='center', va='bottom', color=ax.patches[0].get_facecolor(),\n",
    "            size=8)\n",
    "    \n",
    "ax.axvline(tn_svi_nod[selected_variables_tn[0]].mean() - tn_svi_nod[selected_variables_tn[0]].std(), color='r', linestyle='dashed', linewidth=1)\n",
    "ax.text(tn_svi_nod[selected_variables_tn[0]].mean() - tn_svi_nod[selected_variables_tn[0]].std(), ax.get_ylim()[1], 'Mean-1Sd', color='r', va='bottom', ha='right')\n",
    "\n",
    "\n",
    "# mean\n",
    "ax.axvline(tn_svi_nod[selected_variables_tn[0]].mean(), color='r', linestyle='dashed', linewidth=1)\n",
    "ax.text(tn_svi_nod[selected_variables_tn[0]].mean(), ax.get_ylim()[1], 'Mean', color='r', va='bottom', ha='right')\n",
    "\n",
    "\n",
    "ax.axvline(tn_svi_nod[selected_variables_tn[0]].mean() + tn_svi_nod[selected_variables_tn[0]].std(), color='r', linestyle='dashed', linewidth=1)\n",
    "ax.text(tn_svi_nod[selected_variables_tn[0]].mean() + 2*tn_svi_nod[selected_variables_tn[0]].std(), ax.get_ylim()[1], 'Mean+1Sd', color='r', va='bottom', ha='right')\n",
    "\n",
    "\n",
    "# axhline\n",
    "ax.hlines(y=2.6, xmin=0, xmax=tn_svi_nod[selected_variables_tn[0]].mean()+ tn_svi_nod[selected_variables_tn[0]].std(), color='g', linestyle='dashed', linewidth=2)\n",
    "ax.hlines(y=2.4, xmin=0, xmax=tn_svi_nod[selected_variables_tn[0]].mean(), color='g', linestyle='dashed', linewidth=2)\n",
    "ax.hlines(y=2.2, xmin=0, xmax=tn_svi_nod[selected_variables_tn[0]].mean()- tn_svi_nod[selected_variables_tn[0]].std(), color='g', linestyle='dashed', linewidth=2)\n",
    "\n",
    "\n",
    "    \n",
    "# Add labels and a title.\n",
    "ax.set_xlabel('Estimated Value', labelpad=15, color='#333333')\n",
    "ax.set_ylabel('Frequency', labelpad=15, color='#333333')\n",
    "ax.set_title('Disability population distribution', pad=15, color='#333333')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from scipy import spatial\n",
    "import pickle as pickle\n",
    "import gudhi\n",
    "from pylab import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw, ImageChops, ImageFont\n",
    "import shapely.geometry as geom\n",
    "from shapely.ops import unary_union\n",
    "import warnings\n",
    "\n",
    "import invr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_filtered = tn_svi_nod[selected_variables_tn_with_geo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "tn_filtered = tn_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the uniques fips codes\n",
    "fips = tn_filtered['FIPS'].unique()\n",
    "fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select the rows where the variable is below the 50th percentile\n",
    "# variable_1_below_50 = tn_filtered[tn_filtered[selected_variables_tn[1]] < variable_2_percentile_75]\n",
    "\n",
    "# # get the shape\n",
    "# variable_1_below_50.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate adjacent counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adjacent_counties(dataframe,filtration_threshold,variable_name):\n",
    "\n",
    "    \n",
    "    filtered_df = dataframe[dataframe[variable_name] < filtration_threshold]\n",
    "\n",
    "    # Perform a spatial join to find adjacent precincts\n",
    "    adjacent_counties = gpd.sjoin(filtered_df, filtered_df, predicate='intersects', how='left')\n",
    "\n",
    "    # Filter the results to include only the adjacent states\n",
    "    adjacent_counties = adjacent_counties.query('sortedID_left != sortedID_right')\n",
    "\n",
    "    # Group the resulting dataframe by the original precinct Name and create a list of adjacent precinct Name\n",
    "    adjacent_counties = adjacent_counties.groupby('sortedID_left')['sortedID_right'].apply(list).reset_index()\n",
    "\n",
    "    adjacent_counties.rename(columns={'sortedID_left': 'county', 'sortedID_right': 'adjacent'}, inplace=True)\n",
    "\n",
    "    adjacencies_list = adjacent_counties['adjacent'].tolist()\n",
    "    county_list = adjacent_counties['county'].tolist()\n",
    "\n",
    "    merged_df = pd.merge(adjacent_counties, dataframe, left_on='county',right_on='sortedID', how='left')\n",
    "    merged_df = gpd.GeoDataFrame(merged_df, geometry='geometry')\n",
    "\n",
    "    return adjacencies_list,merged_df,county_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_simplicial_complex(adjacent_county_list,county_list):\n",
    "    max_dimension = 3\n",
    "\n",
    "    V = []\n",
    "    V = invr.incremental_vr(V, adjacent_county_list, max_dimension,county_list)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2img(fig):\n",
    "     #convert matplot fig to image and return it\n",
    "\n",
    "     buf = io.BytesIO()\n",
    "     fig.savefig(buf)\n",
    "     buf.seek(0)\n",
    "     img = Image.open(buf)\n",
    "     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simplicial_complex(dataframe,V):\n",
    "\n",
    "    #city centroids\n",
    "    city_coordinates = {city.sortedID: np.array((city.geometry.centroid.x, city.geometry.centroid.y)) for _, city in dataframe.iterrows()}\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_axis_off() \n",
    "\n",
    "    # Plot the \"wyoming_svi\" DataFrame\n",
    "    dataframe.plot(ax=ax, edgecolor='black', linewidth=0.3, color=\"white\")\n",
    "\n",
    "    # Plot the centroid of the large square with values\n",
    "    # for i, row in dataframe.iterrows():\n",
    "    #     centroid = row['geometry'].centroid\n",
    "    #     # text_to_display = f\"FIPS: {row['FIPS']}\\nFilteration: {row['EP_SNGPNT']}\"\n",
    "    #     plt.text(centroid.x, centroid.y, str(row['FIPS']), fontsize=8, ha='center', color=\"black\")\n",
    "    #     # plt.text(centroid.x, centroid.y, text_to_display, fontsize=10, ha='center', color=\"black\")\n",
    "\n",
    "    for edge_or_traingle in V:\n",
    "\n",
    "        \n",
    "        if len(edge_or_traingle) == 2:\n",
    "            # Plot an edge\n",
    "            ax.plot(*zip(*[city_coordinates[vertex] for vertex in edge_or_traingle]), color='red', linewidth=1)\n",
    "            # img = fig2img(fig)\n",
    "            # list_gif.append(img)\n",
    "        elif len(edge_or_traingle) == 3:\n",
    "            # Plot a triangle\n",
    "            ax.add_patch(plt.Polygon([city_coordinates[vertex] for vertex in edge_or_traingle], color='green', alpha=0.2))\n",
    "            # img = fig2img(fig)\n",
    "            # list_gif.append(img)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    #return list_gif #deleted  plot_simplicial_complex(dataframe,V,list_gif) --> list part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple variable code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_and_threshold = {selected_variables_tn[0]: variable_1_percentile_50, selected_variables_tn[1]: variable_2_percentile_50, selected_variables_tn[2]: variable_3_percentile_50, selected_variables_tn[3]: variable_4_percentile_50, selected_variables_tn[4]: variable_5_percentile_50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a empty dictionary\n",
    "edges_and_traingles_for_each_variable_below_50th_percentile = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name, threshold in selected_variables_and_threshold.items():\n",
    "\n",
    "    # Sorting based on the variable and selecting only the FIPS and the variable columns is important\n",
    "    # Also we need to keep  the dataframe sorted based on the variable\n",
    "\n",
    "    df_one_variable = tn_filtered[['FIPS',variable_name, 'geometry']]\n",
    "\n",
    "    # # Sorting the DataFrame based on the 'rate' column\n",
    "    df_one_variable = df_one_variable.sort_values(by=variable_name)\n",
    "    df_one_variable['sortedID'] = range(len(df_one_variable))\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    df_one_variable = gpd.GeoDataFrame(df_one_variable, geometry='geometry')\n",
    "    df_one_variable.crs = \"EPSG:3395\"  # This is a commonly used projected CRS\n",
    "\n",
    "\n",
    "    # print(df_one_variable.head(100))\n",
    "\n",
    "    adjacencies_list,adjacent_counties_df,county_list = generate_adjacent_counties(df_one_variable,threshold,variable_name)\n",
    "\n",
    "    # create a dictionary adjacent_counties_df column county as key and column adjacent as value(to avoid NULL adjacencies error)\n",
    "    adjacent_counties_dict = dict(zip(adjacent_counties_df['county'],adjacent_counties_df['adjacent']))\n",
    "\n",
    "    # this take only counties that have adjacent counties\n",
    "    county_list = adjacent_counties_df['county'].tolist()\n",
    "\n",
    "    V = form_simplicial_complex(adjacent_counties_dict,county_list)\n",
    "\n",
    "    # This is a new feature that I added to the code. It creates a new list replace the sorted ID with the FIPS on the V list\n",
    "    # create a new list replace the sorted ID with the FIPS on the V list\n",
    "    V_FIPS = [[df_one_variable.iloc[x]['FIPS'] for x in i] for i in V]\n",
    "    \n",
    "    #add V list to the edges_and_traingles_for_each_variable dictionary with the key as the variable name\n",
    "    edges_and_traingles_for_each_variable_below_50th_percentile[variable_name] = V_FIPS\n",
    "\n",
    "    # # # store the list of images for each variable\n",
    "    # # list_img = []\n",
    "\n",
    "    # # plot the simplicial complex\n",
    "    print(f\"Plotting simplicial complex for {variable_name} variable at threshold {threshold}\")\n",
    "    plot_simplicial_complex(df_one_variable,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_and_threshold = {selected_variables_tn[0]: variable_1_percentile_75, selected_variables_tn[1]: variable_2_percentile_75, selected_variables_tn[2]: variable_3_percentile_75, selected_variables_tn[3]: variable_4_percentile_75, selected_variables_tn[4]: variable_5_percentile_75}\n",
    "# create a empty dictionary\n",
    "edges_and_traingles_for_each_variable_below_75th_percentile = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name, threshold in selected_variables_and_threshold.items():\n",
    "\n",
    "    # Sorting based on the variable and selecting only the FIPS and the variable columns is important\n",
    "    # Also we need to keep  the dataframe sorted based on the variable\n",
    "\n",
    "    df_one_variable = tn_filtered[['FIPS',variable_name, 'geometry']]\n",
    "\n",
    "    # # Sorting the DataFrame based on the 'rate' column\n",
    "    df_one_variable = df_one_variable.sort_values(by=variable_name)\n",
    "    df_one_variable['sortedID'] = range(len(df_one_variable))\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    df_one_variable = gpd.GeoDataFrame(df_one_variable, geometry='geometry')\n",
    "    df_one_variable.crs = \"EPSG:3395\"  # This is a commonly used projected CRS\n",
    "\n",
    "    adjacencies_list,adjacent_counties_df,county_list = generate_adjacent_counties(df_one_variable,threshold,variable_name)\n",
    "\n",
    "    # create a dictionary adjacent_counties_df column county as key and column adjacent as value(to avoid NULL adjacencies error)\n",
    "    adjacent_counties_dict = dict(zip(adjacent_counties_df['county'],adjacent_counties_df['adjacent']))\n",
    "\n",
    "    # this take only counties that have adjacent counties\n",
    "    county_list = adjacent_counties_df['county'].tolist()\n",
    "\n",
    "    V = form_simplicial_complex(adjacent_counties_dict,county_list)\n",
    "\n",
    "    # This is a new feature that I added to the code. It creates a new list replace the sorted ID with the FIPS on the V list\n",
    "    # create a new list replace the sorted ID with the FIPS on the V list\n",
    "    V_FIPS = [[df_one_variable.iloc[x]['FIPS'] for x in i] for i in V]\n",
    "\n",
    "    #add V list to the edges_and_traingles_for_each_variable dictionary with the key as the variable name\n",
    "    edges_and_traingles_for_each_variable_below_75th_percentile[variable_name] = V_FIPS\n",
    "\n",
    "    # # # store the list of images for each variable\n",
    "    # # list_img = []\n",
    "\n",
    "    # # plot the simplicial complex\n",
    "    print(f\"Plotting simplicial complex for {variable_name} variable at threshold {threshold}\")\n",
    "    plot_simplicial_complex(df_one_variable,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables_and_threshold = {selected_variables_tn[0]: variable_1_percentile_90, selected_variables_tn[1]: variable_2_percentile_90, selected_variables_tn[2]: variable_3_percentile_90, selected_variables_tn[3]: variable_4_percentile_90, selected_variables_tn[4]: variable_5_percentile_90}\n",
    "# create a empty dictionary\n",
    "edges_and_traingles_for_each_variable_below_90th_percentile = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name, threshold in selected_variables_and_threshold.items():\n",
    "\n",
    "    # Sorting based on the variable and selecting only the FIPS and the variable columns is important\n",
    "    # Also we need to keep  the dataframe sorted based on the variable\n",
    "\n",
    "    df_one_variable = tn_filtered[['FIPS',variable_name, 'geometry']]\n",
    "\n",
    "    # # Sorting the DataFrame based on the 'rate' column\n",
    "    df_one_variable = df_one_variable.sort_values(by=variable_name)\n",
    "    df_one_variable['sortedID'] = range(len(df_one_variable))\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    df_one_variable = gpd.GeoDataFrame(df_one_variable, geometry='geometry')\n",
    "    df_one_variable.crs = \"EPSG:3395\"  # This is a commonly used projected CRS\n",
    "\n",
    "    adjacencies_list,adjacent_counties_df,county_list = generate_adjacent_counties(df_one_variable,threshold,variable_name)\n",
    "\n",
    "    # create a dictionary adjacent_counties_df column county as key and column adjacent as value(to avoid NULL adjacencies error)\n",
    "    adjacent_counties_dict = dict(zip(adjacent_counties_df['county'],adjacent_counties_df['adjacent']))\n",
    "\n",
    "    # this take only counties that have adjacent counties\n",
    "    county_list = adjacent_counties_df['county'].tolist()\n",
    "\n",
    "    V = form_simplicial_complex(adjacent_counties_dict,county_list)\n",
    "\n",
    "    # This is a new feature that I added to the code. It creates a new list replace the sorted ID with the FIPS on the V list\n",
    "    # create a new list replace the sorted ID with the FIPS on the V list\n",
    "    V_FIPS = [[df_one_variable.iloc[x]['FIPS'] for x in i] for i in V]\n",
    "\n",
    "    #add V list to the edges_and_traingles_for_each_variable dictionary with the key as the variable name\n",
    "    edges_and_traingles_for_each_variable_below_90th_percentile[variable_name] = V_FIPS\n",
    "\n",
    "    # # # store the list of images for each variable\n",
    "    # # list_img = []\n",
    "\n",
    "    # # plot the simplicial complex\n",
    "    print(f\"Plotting simplicial complex for {variable_name} variable at threshold {threshold}\")\n",
    "    plot_simplicial_complex(df_one_variable,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a matrix for the full region(without filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_regions_variable_1 = []\n",
    "selected_regions_variable_2 = []\n",
    "selected_regions_variable_3 = []\n",
    "selected_regions_variable_4 = []\n",
    "selected_regions_variable_5 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop decides which percentile we are selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the dictionary and for each variable create a list of edges\n",
    "for variable_name, V_FIPS in edges_and_traingles_for_each_variable_below_75th_percentile.items():\n",
    "# for variable_name, V_FIPS in edges_and_traingles_for_each_variable_below_mean_plus_1sd.items():\n",
    "    for set in V_FIPS:\n",
    "        if len(set) == 2 or len(set) == 3:\n",
    "            # if variable is EP_DISABL\n",
    "            if variable_name == selected_variables_tn[0]:\n",
    "                #check if the edge(both values) is not already in the list\n",
    "                for vertice in set:\n",
    "                    if vertice not in selected_regions_variable_1:\n",
    "                        selected_regions_variable_1.append(vertice)\n",
    "            elif variable_name == selected_variables_tn[1]:\n",
    "                for vertice in set:\n",
    "                    if vertice not in selected_regions_variable_2:\n",
    "                        selected_regions_variable_2.append(vertice)\n",
    "            elif variable_name == selected_variables_tn[2]:\n",
    "                for vertice in set:\n",
    "                    if vertice not in selected_regions_variable_3:\n",
    "                        selected_regions_variable_3.append(vertice)\n",
    "            elif variable_name == selected_variables_tn[3]:\n",
    "                for vertice in set:\n",
    "                    if vertice not in selected_regions_variable_4:\n",
    "                        selected_regions_variable_4.append(vertice)\n",
    "            elif variable_name == selected_variables_tn[4]:\n",
    "                for vertice in set:\n",
    "                    if vertice not in selected_regions_variable_5:\n",
    "                        selected_regions_variable_5.append(vertice)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this set of variables needed - specially STCNTY to identify the county\n",
    "selected_variables_tn_with_censusinfo = ['FIPS','STCNTY','EP_DISABL', 'EP_NOHSDP', 'EP_PCI', 'EP_MOBILE', 'EP_POV','NOD_Rate','geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a filtered dataframe with the selected regions\n",
    "filtered_df_census = tn_svi_nod[selected_variables_tn_with_censusinfo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique counties\n",
    "unique_counties = tn_svi_nod['STCNTY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving selected countoes as a okl file fir all the variables(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the unique counties and get the number of rows for each county\n",
    "i=0\n",
    "for county in unique_counties:\n",
    "    print(f\"County: {county}\")\n",
    "\n",
    "    # create a temp_df with the selected county\n",
    "    temp_census = filtered_df_census[filtered_df_census['STCNTY'] == county]\n",
    "\n",
    "    # filter the filtered_df_census dataframe to only include the selected census for each selected variable\n",
    "    # FIPS includes the census info\n",
    "    variable_1_selected_census = temp_census[temp_census['FIPS'].isin(selected_regions_variable_1)][['FIPS',selected_variables_tn[0],'geometry']]\n",
    "    variable_2_selected_census = temp_census[temp_census['FIPS'].isin(selected_regions_variable_2)][['FIPS',selected_variables_tn[1],'geometry']]\n",
    "    variable_3_selected_census = temp_census[temp_census['FIPS'].isin(selected_regions_variable_3)][['FIPS',selected_variables_tn[2],'geometry']]\n",
    "    variable_4_selected_census = temp_census[temp_census['FIPS'].isin(selected_regions_variable_4)][['FIPS',selected_variables_tn[3],'geometry']]\n",
    "    variable_5_selected_census = temp_census[temp_census['FIPS'].isin(selected_regions_variable_5)][['FIPS',selected_variables_tn[4],'geometry']]\n",
    "\n",
    "    # create a new column in df that contains the x and y coordinates of the centroid of each polygon\n",
    "    variable_1_selected_census['coords'] = variable_1_selected_census['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    variable_1_selected_census['coords'] = [coords[0] for coords in variable_1_selected_census['coords']]\n",
    "\n",
    "    variable_2_selected_census['coords'] = variable_2_selected_census['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    variable_2_selected_census['coords'] = [coords[0] for coords in variable_2_selected_census['coords']]\n",
    "\n",
    "    variable_3_selected_census['coords'] = variable_3_selected_census['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    variable_3_selected_census['coords'] = [coords[0] for coords in variable_3_selected_census['coords']]\n",
    "\n",
    "    variable_4_selected_census['coords'] = variable_4_selected_census['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    variable_4_selected_census['coords'] = [coords[0] for coords in variable_4_selected_census['coords']]\n",
    "\n",
    "    variable_5_selected_census['coords'] = variable_5_selected_census['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    variable_5_selected_census['coords'] = [coords[0] for coords in variable_5_selected_census['coords']]\n",
    "\n",
    "\n",
    "    # # plotting the centroids\n",
    "    # plt.scatter(*zip(*variable_1_selected_census['coords']), s=10, c='r')\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.scatter(*zip(*variable_2_selected_census['coords']), s=10, c='r')\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.scatter(*zip(*variable_3_selected_census['coords']), s=10, c='r')\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.scatter(*zip(*variable_4_selected_census['coords']), s=10, c='r')\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.scatter(*zip(*variable_5_selected_census['coords']), s=10, c='r')\n",
    "    # plt.show()\n",
    "\n",
    "    # create a dictionary with variable name as key and the data as value for all the selected regions\n",
    "    selected_coordinates_dic = {selected_variables_tn[0]: variable_1_selected_census, selected_variables_tn[1]: variable_2_selected_census, selected_variables_tn[2]: variable_3_selected_census, selected_variables_tn[3]: variable_4_selected_census, selected_variables_tn[4]: variable_5_selected_census}\n",
    "\n",
    "\n",
    "    # save the dictionary to a pickle file\n",
    "    with open(f'./results/selected coordinates for each county - percentiles/selected_coordinates_dic_{county}.pkl', 'wb') as f:\n",
    "        pickle.dump(selected_coordinates_dic, f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
